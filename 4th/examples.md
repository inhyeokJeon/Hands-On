examples
================
1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요 ?
> 경사 하강법을 사용한다. 경사하강법은 특성수에 미간하지 않기 때문이다.
> 정규 방정식이나 SVD 방법은 특성 개수에 따라 매우 빠르게 증가하기 떄문에 사용하기 어렵다.
2. 훈련 세트에 있는 특성들이 각기 아주 다른 스케일을 가지고 있씁니다. 이런 데이터에 잘 작동하지 않는 알고리즘은 무엇일까요? 그 이유는 무엇일까요? 이 문제를 해결할 수 있을까요?
> 특성의 스케일이 매우 다르면 비용 함수는 길쭉한 타원 모양의 그릇 형태가 된다. 그래서 **경사 하강법 알고리즘**이 수렴하는데 오랜 시간이 걸린다.
> 이를 해결하기 위해서는 모델 훈련 전 스케일을 조절해야 한다. 
> 정규 방정식이나 SVD 방법은 스케일 조정 없이도 잘 작동함.
3. 경사 하강법으로 로지스틱 회귀 모델을 훈련시킬 때 지역 최솟값에 갇힐 가능성이 있을까요?
> 비용 함수가 볼록 함수이므로 경사 하강법을 사용하였을때 전역 최솟값을 찾는 것을 보장한다.
4. 충분히 오랫동안 실행되면 모든 경사 하강법 알고리즘이 같은 모델을 만들어낼까요? 
> 아니요, 지역 최솟값에 갇히는 모델도 있을 것이며, 확률적 경사하강법 같은경우는 변동이커서 아닐 수 있습니다.
> 최적화할 함수가 (선형 회귀나 로지스틱 회귀처럼) 볼록 함수이고 학습률이 너무 크지 않다고 가정ㅎ마ㅕㄴ 모든 경사 하강법 알고리즘이 전역 최적값에 도달한다. 
5. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있따면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결 할 수 있나요?
> 점진적으로 오차가 상승하고 있기에 인지하지못하고 전역 최솟값을 벗어나고 있다. 
> 이를 해결하기 위해 일정 시간이 지나면 최솟값으로 계산하도록 한다.
> **학습률이 너무 높고 알고리즘이 발산하는 것이다. 훈련에러가 올라가고 이문제가 확실하면 학습률을 낮추어야합니다. 
> ** 훈련 에러가 올라가지 않는다면 모델이 훈련 세트에 과대적합되어 있는 것이므로 훈련을 멈추어야 한다.
6. 검증 오차가 상승하면 미니배치 경사 하강법을 즉시 중단하는 것이 좋은 방법인가요?
> 아니요 적당한 학습 스케줄을 사용해야한다.
> ** 무작위성 때문에 확률적 경사 하강법이나 미니배치 경사 하강법 모두 매 훈련 반복마다 학습의 진전을 보장하지 못합니다. 검증 에러가 상승될 떄 훈련을 즉시 멈춘다면 최적점에 도달하기 전에 너무 일찍 멈추게 될 지 모른다.
> ** 그 다음에 방법은 정지적으로 모델을 저장하고 오랫동안 진전이 없을때, 저장된 것 중 가장 좋은 모델로 복원하는 것이다.
7. (우리가 언급한 것 중에서) 어던 경사 하강법 알고리즘이 가장 빠르게 최적 솔루션의 주변에 도달할까요? 실제로 수렴하는 것은 어떤 것인가요? 다른 방법들도 수렴하게 만들 수 있나요?
> 확률적 경사 하강법 , 한번에 하나의 훈련 샘플만 사용하기 때문에 훈련 반복이 가장 빠른다. 그래서 가장 먼저 전역 최적점 근처에 도달한다.
> 그다음 미니배치 경사법 GPU를 사용해 성능이 향상되었기 떄문에.
8. 다항 회귀르 사용했을 때 학습 곡선을 보니 훈련 오차와 검증 오차 사이에 간격이 큽니다 무슨 일이 생긴 걸까요? 이 문제를 해결하는 세 가지 방법은 무엇인가요?
> 훈련 데이터에서의 모델 성능이 검증 데이터에서보다 훨씬 낫다는 뜻이고, 이는 과대적합 되었다는 뜻이다.
> 검증 오차가 훈련 오차에 근접할 때까지 더 많은 훈련 데이터를 추가하는 것이다.
> 모델을 규제하는 것이다.
> 다항식의 차수를 감소 시킨다.
9. 릿지 회귀를 사용했을때 훈련 오차와 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 a를 증가시켜야 할까요, 아니면 줄여야 할까요?
> 과소적합되었을 가능성이 매우 높다. 즉 , 높은 편향을 가진 모델이다. 따라서 규제 하이퍼 파라미터 a를 감소시켜야 한다.
10. 다음과 같이 사용해야 하는 이유는?
>+ 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지회귀
> > 규제가 약간 있는 것이 대부분의 경우에 좋기 때문에 선형회귀를 피하고 다른 회귀 모델을 사용한다.
>+ 릿지 회귀 대신 라쏘 회귀
> > 쓰이는 특성이 몇개 뿐이라고 의심되면 라쏘 회귀가 낫다.
>+ 라쏘 회귀 대신 엘라스틱넷
> > 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 보통 라쏘가 문제를 일으키므로 엘라스틱넷을 선호한다.
11. 사진을 낮과 밤, 실내와 실외로 분류하려 합니다. 두 개의 로지스틱 회귀 분류기를 만들어야 할까요, 아니면 하나의 소프트맥스 회귀 분류기를 만들어야 할까요?
> 두개의 로지스틱 회귀 분류기를 훈련시켜야 한다.
12. 조기 종료를 사용한 배치 겨사 하강법으로 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)